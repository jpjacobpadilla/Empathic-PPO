{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "91CuURHUmUv0"
      },
      "outputs": [],
      "source": [
        "%pip install -q trl==0.10.1 tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3huLAeM1IDV"
      },
      "source": [
        "**Mount drive to get conversations and empathy scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3__PtGcsziSe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "BATCH = 1\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "conversations = Path(f'/content/drive/My Drive/conversations/batch_{BATCH}')\n",
        "\n",
        "print(f'Found {len(list(conversations.glob(\"*.json\"))):,} files in batch {BATCH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_x1XLbejzeiK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with (conversations / 'empathy_scores.json').open('r', encoding='utf8') as f:\n",
        "    empathy_scores = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Oo0RewIaTan"
      },
      "source": [
        "**Prepare training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Qpvj7kTOb5W_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZiBo5T3bFHp"
      },
      "outputs": [],
      "source": [
        "MAX_QUERY_LEN = 1024\n",
        "\n",
        "def extract_ppo_training_samples(convo_data, reward, tokenizer):\n",
        "    samples = []\n",
        "    messages = []\n",
        "\n",
        "    for message in convo_data:\n",
        "        role = message[\"role\"]\n",
        "        content = message[\"content\"].strip()\n",
        "\n",
        "        messages.append({\n",
        "            \"role\": role,\n",
        "            \"content\": content\n",
        "        })\n",
        "\n",
        "        if role == \"assistant\":\n",
        "            prompt = tokenizer.apply_chat_template(messages[:-1], add_generation_prompt=True, tokenize=False)\n",
        "            response = content\n",
        "\n",
        "            truncated_prompt = prompt[-MAX_QUERY_LEN:]\n",
        "            truncated_response = response[-MAX_QUERY_LEN:]\n",
        "\n",
        "            query_tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_QUERY_LEN)\n",
        "            response_tokens = tokenizer(response, return_tensors=\"pt\", truncation=True, max_length=MAX_QUERY_LEN)\n",
        "\n",
        "            samples.append({\n",
        "                \"query\": prompt,\n",
        "                \"query_raw\": messages[:-1],\n",
        "                \"response\": response,\n",
        "                \"truncated_query\": truncated_prompt,\n",
        "                \"truncated_response\": truncated_response,\n",
        "                \"reward\": reward,\n",
        "                \"input_ids\": query_tokens.input_ids.to('cuda'),\n",
        "                \"response_ids\": response_tokens.input_ids.to('cuda')\n",
        "            })\n",
        "\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIVLvYb4aSuB",
        "outputId": "4b19ceab-d8dc-47bb-93a5-8c1a7f0cdaad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing conversations: 100%|██████████| 1001/1001 [00:57<00:00, 17.27it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "training_data = []\n",
        "\n",
        "def normalize_reward(score: int) -> float:\n",
        "    return score / 10.0\n",
        "\n",
        "for path in tqdm(list(conversations.glob('*.json')), desc=\"Processing conversations\"):\n",
        "    if path.stem == 'empathy_scores':\n",
        "        continue\n",
        "\n",
        "    with path.open('r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    training_data.extend(extract_ppo_training_samples(data['convo'], normalize_reward(empathy_scores[path.stem]), tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HcdBfG_9nvTd"
      },
      "outputs": [],
      "source": [
        "del tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t49DuHT1Zpa2",
        "outputId": "b92ff87b-5d6b-41db-9efb-818e0f7e7116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b1A1PjrMjOeC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpUrTNDQeh2X"
      },
      "source": [
        "**Training loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuBwFseSejAw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
        "import gc\n",
        "import os\n",
        "\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=model_id,\n",
        "    learning_rate=1e-5,\n",
        "    batch_size=1,\n",
        "    mini_batch_size=1,\n",
        "    cliprange=0.2,\n",
        "    kl_penalty=\"kl\",\n",
        "    init_kl_coef=0.05\n",
        ")\n",
        "\n",
        "trainer = PPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=ref_model,\n",
        "    tokenizer=tokenizer,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "\n",
        "def train_on_sample(sample):\n",
        "    query_ids = sample[\"input_ids\"]\n",
        "    response_ids = sample[\"response_ids\"]\n",
        "    reward = torch.tensor(sample[\"reward\"], device=device)\n",
        "\n",
        "    trainer.step([query_ids[0]], [response_ids[0]], [reward])\n",
        "\n",
        "    del query_ids, response_ids, reward\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "for idx, sample in enumerate(training_data, start=1):\n",
        "    print(f\"Sample {idx}/{len(training_data)}\")\n",
        "    train_on_sample(sample)\n",
        "\n",
        "    if idx % 100 == 0:\n",
        "        print(f\"Saving checkpoint after sample {idx}\")\n",
        "        try:\n",
        "            save_dir = f\"/content/ppo_model_final_2_{idx}\"\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "            trainer.model.save_pretrained(save_dir)\n",
        "            trainer.tokenizer.save_pretrained(save_dir)\n",
        "            torch.save(trainer.model.v_head.state_dict(), os.path.join(save_dir, \"value_head.pt\"))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving: {e}\")\n",
        "\n",
        "        break\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Training loop 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
        "import gc\n",
        "import os\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map={\"\": device}, \n",
        "    max_memory={0: \"30GB\"} \n",
        ")\n",
        "\n",
        "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map={\"\": device} \n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "ref_model.gradient_checkpointing_enable()\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=model_id,\n",
        "    learning_rate=5e-6, \n",
        "    batch_size=1,\n",
        "    mini_batch_size=1,\n",
        "    cliprange=0.1, \n",
        "    cliprange_value=0.1,\n",
        "    kl_penalty=\"kl\",\n",
        "    init_kl_coef=0.1, \n",
        "    target_kl=0.5, \n",
        "    gradient_accumulation_steps=1,\n",
        "    vf_coef=0.1,  \n",
        "    max_grad_norm=0.5\n",
        ")\n",
        "\n",
        "trainer = PPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=ref_model,\n",
        "    tokenizer=tokenizer,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "def preprocess_sample(sample):\n",
        "    if isinstance(sample[\"input_ids\"], torch.Tensor):\n",
        "        sample[\"input_ids\"] = sample[\"input_ids\"].to(device)\n",
        "    else:\n",
        "        sample[\"input_ids\"] = torch.tensor(sample[\"input_ids\"], device=device)\n",
        "        \n",
        "    if isinstance(sample[\"response_ids\"], torch.Tensor):\n",
        "        sample[\"response_ids\"] = sample[\"response_ids\"].to(device)\n",
        "    else:\n",
        "        sample[\"response_ids\"] = torch.tensor(sample[\"response_ids\"], device=device)\n",
        "        \n",
        "    if isinstance(sample[\"reward\"], torch.Tensor):\n",
        "        sample[\"reward\"] = sample[\"reward\"].to(device)\n",
        "    else:\n",
        "        sample[\"reward\"] = torch.tensor(sample[\"reward\"], device=device)\n",
        "        \n",
        "    return sample\n",
        "\n",
        "def train_on_sample(sample):\n",
        "    try:\n",
        "        # Ensure all tensors are on the same device\n",
        "        sample = preprocess_sample(sample)\n",
        "        \n",
        "        query_ids = sample[\"input_ids\"]\n",
        "        response_ids = sample[\"response_ids\"]\n",
        "        reward = sample[\"reward\"]\n",
        "        \n",
        "        # Process with mixed precision\n",
        "        with torch.cuda.amp.autocast():\n",
        "            stats = trainer.step([query_ids[0]], [response_ids[0]], [reward])\n",
        "        \n",
        "        # Explicit cleanup\n",
        "        del query_ids, response_ids, reward\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        \n",
        "        return stats\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e):\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            print(\"OOM error, skipping sample\")\n",
        "            return None\n",
        "        else:\n",
        "            print(f\"Runtime error: {e}\")\n",
        "            raise e\n",
        "\n",
        "consecutive_errors = 0\n",
        "max_consecutive_errors = 5\n",
        "\n",
        "for idx, sample in enumerate(training_data, start=1):\n",
        "    print(f\"Sample {idx}/{len(training_data)}\")\n",
        "    \n",
        "    input_len = len(sample[\"input_ids\"][0]) if isinstance(sample[\"input_ids\"], list) else sample[\"input_ids\"].shape[1]\n",
        "    response_len = len(sample[\"response_ids\"][0]) if isinstance(sample[\"response_ids\"], list) else sample[\"response_ids\"].shape[1]\n",
        "    \n",
        "    if input_len + response_len > 2048:\n",
        "        print(f\"Skipping oversized sample {idx}\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        stats = train_on_sample(sample)\n",
        "        consecutive_errors = 0 \n",
        "        \n",
        "        # Force garbage collection more frequently\n",
        "        if idx % 5 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "        \n",
        "        # Save less frequently to reduce memory pressure\n",
        "        if idx % 250 == 0:\n",
        "            print(f\"Saving checkpoint after sample {idx}\")\n",
        "            try:\n",
        "                torch.cuda.empty_cache()  # Clear cache before saving\n",
        "                save_dir = f\"/content/ppo_model_final_4_{idx}\"\n",
        "                os.makedirs(save_dir, exist_ok=True)\n",
        "                trainer.model.save_pretrained(save_dir)\n",
        "                trainer.tokenizer.save_pretrained(save_dir)\n",
        "                torch.save(trainer.model.v_head.state_dict(), os.path.join(save_dir, \"value_head.pt\"))\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving: {e}\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error on sample {idx}: {e}\")\n",
        "        # Try to recover\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        \n",
        "        consecutive_errors += 1\n",
        "        if consecutive_errors >= max_consecutive_errors:\n",
        "            print(f\"Too many consecutive errors ({consecutive_errors}). Stopping training.\")\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
